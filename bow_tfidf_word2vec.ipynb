{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQAUq6jvXNh8r/fC+OvM9r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/p1x3lp1r6te/23124052-CSOC-IG/blob/main/bow_tfidf_word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wrxhjCgZL2fz"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "\n",
        "    # Remove non-alphabetic characters and convert to lowercase\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text).lower()\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Lemmatize the words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    # Combine words back into a single string\n",
        "    preprocessed_text = ' '.join(words)\n",
        "\n",
        "    return preprocessed_text\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_svm_with_representations(train_data, test_data, representation):\n",
        "    if representation == 'bow':\n",
        "        vectorizer = CountVectorizer()\n",
        "    elif representation == 'tfidf':\n",
        "        vectorizer = TfidfVectorizer()\n",
        "    else:\n",
        "        raise ValueError(\"Invalid representation. Choose 'bow' or 'tfidf'.\")\n",
        "\n",
        "    X_train = vectorizer.fit_transform(train_data)\n",
        "    X_test = vectorizer.transform(test_data)\n",
        "\n",
        "    clf = SVC()\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    return y_pred\n",
        "\n",
        "# TF-IDF\n",
        "#y_pred_tfidf = train_svm_with_representations(X_train, X_test, 'tfidf')\n",
        "#accuracy_tfidf = accuracy_score(y_test, y_pred_tfidf)\n",
        "#print(accuracy_tfidf)"
      ],
      "metadata": {
        "id": "8MNAaP1vN96N"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word2vec_embeddings(data):\n",
        "    tokenized_sentences = [sentence.split() for sentence in data]\n",
        "    model = Word2Vec(tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "    embeddings = np.array([np.mean([model.wv[word] for word in sentence], axis=0) for sentence in tokenized_sentences])\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "def train_svm_with_word2vec(train_data, test_data):\n",
        "    X_train = get_word2vec_embeddings(train_data)\n",
        "    X_test = get_word2vec_embeddings(test_data)\n",
        "\n",
        "    clf = SVC()\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    return y_pred\n",
        "\n",
        "# Word2Vec\n",
        "#y_pred_word2vec = train_svm_with_word2vec(X_train, X_test)\n",
        "#accuracy_word2vec = accuracy_score(y_test, y_pred_word2vec)\n",
        "#print(accuracy_word2vec)"
      ],
      "metadata": {
        "id": "5P2utzvPKKSp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SDghBkUpKkzV"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}