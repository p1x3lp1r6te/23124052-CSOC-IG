{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwxFb39o03XWf1VshTWcWV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/p1x3lp1r6te/23124052-CSOC-IG/blob/main/text_preproccessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azICvDkN96hi",
        "outputId": "e1ae9c49-000a-4ab4-bd67-fc8e69c61170"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tokenization', 'is', 'an', 'essential', 'step', 'in', 'NLP', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Tokenization is an essential step in NLP.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4TYwS7Z-cbJ",
        "outputId": "3433ac6d-1a40-4eeb-94ed-528a0933ec35"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tokenization', 'essential', 'step', 'NLP', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Text preprocessing cleaning and transforming raw text data into a \\\n",
        "format suitable for analysis and machine learning models. \\\n",
        "There are 1234 examples in this text.\"\n",
        "\n",
        "text = \"Text preprocessing cleaning and transforming raw text data into a \\\n",
        "format suitable for analysis and machine learning models. \\\n",
        "There are 1234 examples in this text.\"\n",
        "\n",
        "import re\n",
        "text = re.sub(r'\\d+', '', text)\n",
        "print(text)\n",
        "print(\"\\n - -\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6L-es5i-8kH",
        "outputId": "5585dc08-fa79-4e5a-8518-aaf930a18ad2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text preprocessing cleaning and transforming raw text data into a format suitable for analysis and machine learning models. There are  examples in this text.\n",
            "\n",
            " - -\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "words = [\"running\", \"flies\", \"studies\", \"played\", \"better\"]\n",
        "\n",
        "stemmed = [stemmer.stem(word) for word in words]\n",
        "print(\"Stemming:\", stemmed)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxGakzBL_4Gr",
        "outputId": "7d52788c-2fc9-4621-9a6c-ed1be82fe348"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming: ['run', 'fli', 'studi', 'play', 'better']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # for extended lemmatizer support\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [\"running\", \"flies\", \"studies\", \"played\", \"better\"]\n",
        "\n",
        "lemmatized = [lemmatizer.lemmatize(word, pos='v') for word in words]  # 'v' = verb\n",
        "print(\"Lemmatization:\", lemmatized)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HP6YeNFPDQRR",
        "outputId": "c526d466-2c91-43cf-8234-74965931e94c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatization: ['run', 'fly', 'study', 'play', 'better']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "print(\"Stemming:\")\n",
        "print(stemmed_tokens)\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "print(\"Lemmatization:\")\n",
        "print(lemmatized_tokens)\n",
        "print(\"\\n---\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SQbw2CpDTwz",
        "outputId": "cd0b7af5-2b2c-4b7b-aede-f514cf990222"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming:\n",
            "['text', 'preprocess', 'clean', 'and', 'transform', 'raw', 'text', 'data', 'into', 'a', 'format', 'suitabl', 'for', 'analysi', 'and', 'machin', 'learn', 'model', '.', 'there', 'are', 'exampl', 'in', 'thi', 'text', '.']\n",
            "Lemmatization:\n",
            "['Text', 'preprocessing', 'cleaning', 'and', 'transforming', 'raw', 'text', 'data', 'into', 'a', 'format', 'suitable', 'for', 'analysis', 'and', 'machine', 'learning', 'model', '.', 'There', 'are', 'example', 'in', 'this', 'text', '.']\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install pyspellchecker\n",
        "\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "text_with_typos = \"Ths is an example sentennce with sspellinng mstakes.\"\n",
        "\n",
        "# Create a SpellChecker instance\n",
        "spell = SpellChecker()\n",
        "# Tokenize the text into words\n",
        "words = text_with_typos.split()\n",
        "# Identify misspelled words\n",
        "misspelled = spell.unknown(words)\n",
        "# Correct misspelled words\n",
        "corrected_text = [spell.correction(word) if word in misspelled else word for word in words]\n",
        "# Join the corrected words back into a string\n",
        "corrected_text = ' '.join(corrected_text)\n",
        "\n",
        "print(corrected_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ClR3cdgEAzZ",
        "outputId": "96224f82-8305-408c-847a-e0a8641300cc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.8.3-py3-none-any.whl.metadata (9.5 kB)\n",
            "Downloading pyspellchecker-0.8.3-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.8.3\n",
            "Ths is an example sentence with spelling mistakes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "print(pos_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxFYe1VPE0IE",
        "outputId": "8d48994d-ad13-4b05-8c0e-6a6427da6a75"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Text', 'NNP'), ('preprocessing', 'VBG'), ('cleaning', 'NN'), ('and', 'CC'), ('transforming', 'VBG'), ('raw', 'JJ'), ('text', 'NN'), ('data', 'NNS'), ('into', 'IN'), ('a', 'DT'), ('format', 'NN'), ('suitable', 'JJ'), ('for', 'IN'), ('analysis', 'NN'), ('and', 'CC'), ('machine', 'NN'), ('learning', 'NN'), ('models', 'NNS'), ('.', '.'), ('There', 'EX'), ('are', 'VBP'), ('examples', 'NNS'), ('in', 'IN'), ('this', 'DT'), ('text', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import FreqDist\n",
        "\n",
        "words = word_tokenize(text)\n",
        "word_freq = FreqDist(words)\n",
        "threshold=2\n",
        "filtered_words = [word for word in words if word_freq[word] < threshold]"
      ],
      "metadata": {
        "id": "JkFLn6LTE74y"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\",\n",
        "]\n",
        "\n",
        "# Create a TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the documents\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Get feature names (words) from the vectorizer\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the TF-IDF matrix to a dense array and display the results\n",
        "dense_array = tfidf_matrix.toarray()\n",
        "print(\"TF-IDF Matrix:\")\n",
        "print(dense_array)\n",
        "\n",
        "# Display feature names\n",
        "print(\"\\nFeature Names:\")\n",
        "print(feature_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3mAJch1QtxQ",
        "outputId": "3a173647-8e98-45b3-c713-348af80b5392"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Matrix:\n",
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]\n",
            " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
            "  0.28108867 0.         0.28108867]\n",
            " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
            "  0.26710379 0.51184851 0.26710379]\n",
            " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n",
            "\n",
            "Feature Names:\n",
            "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Sample texts\n",
        "texts = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\",\n",
        "]\n",
        "\n",
        "# Create a tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# Convert texts to sequences\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Define a maximum sequence length\n",
        "max_length = 10\n",
        "\n",
        "# Perform padding or truncation\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# Display the results\n",
        "print(\"Original Sequences:\")\n",
        "print(sequences)\n",
        "print(\"\\nPadded/Truncated Sequences:\")\n",
        "print(padded_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzerIcT9TbSw",
        "outputId": "c0ea2154-31ea-493b-ea23-44940b83c076"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sequences:\n",
            "[[1, 2, 3, 5, 4], [1, 4, 2, 3, 6, 4], [7, 1, 2, 3, 8, 9], [2, 1, 3, 5, 4]]\n",
            "\n",
            "Padded/Truncated Sequences:\n",
            "[[1 2 3 5 4 0 0 0 0 0]\n",
            " [1 4 2 3 6 4 0 0 0 0]\n",
            " [7 1 2 3 8 9 0 0 0 0]\n",
            " [2 1 3 5 4 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F-O8LZkBV3Ma"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}